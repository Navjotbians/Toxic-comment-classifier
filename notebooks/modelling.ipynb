{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import feature_extraction,model_selection,preprocessing, naive_bayes,pipeline, manifold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import sys  \n",
    "sys.path.append('F:/AI/Toxic-comment-classifier/src')\n",
    "from word_embeddings import w_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/processed/processed_stem_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>explan edit made usernam hardcor metallica fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>aww match background colour seemingli stuck th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man realli tri edit war guy constantli rem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ca make real suggest improv wonder section sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>sir hero chanc rememb page</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  toxic  severe_toxic  obscene  threat  insult  \\\n",
       "0           0  0000997932d777bf      0             0        0       0       0   \n",
       "1           1  000103f0d9cfb60f      0             0        0       0       0   \n",
       "2           2  000113f07ec002fd      0             0        0       0       0   \n",
       "3           3  0001b41b1c6bb37e      0             0        0       0       0   \n",
       "4           4  0001d958c54c6e35      0             0        0       0       0   \n",
       "\n",
       "   identity_hate                                       comment_text  \n",
       "0              0  explan edit made usernam hardcor metallica fan...  \n",
       "1              0  aww match background colour seemingli stuck th...  \n",
       "2              0  hey man realli tri edit war guy constantli rem...  \n",
       "3              0  ca make real suggest improv wonder section sta...  \n",
       "4              0                         sir hero chanc rememb page  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fill NA for any missing data \n",
    "df['comment_text'].fillna(\"missing\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "corpus = df['comment_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the date into train test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(corpus,df[labels],test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((119678,), (39893,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>number_of_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>11479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>6306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>5866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>1048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Labels  number_of_comments\n",
       "0          toxic               11479\n",
       "1   severe_toxic                1189\n",
       "2        obscene                6306\n",
       "3         threat                 373\n",
       "4         insult                5866\n",
       "5  identity_hate                1048"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stats of X_train labels\n",
    "counts = []\n",
    "for i in labels:\n",
    "    counts.append((i, y_train[i].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['Labels', 'number_of_comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>number_of_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>toxic</td>\n",
       "      <td>3815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>severe_toxic</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obscene</td>\n",
       "      <td>2143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threat</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>insult</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity_hate</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Labels  number_of_comments\n",
       "0          toxic                3815\n",
       "1   severe_toxic                 406\n",
       "2        obscene                2143\n",
       "3         threat                 105\n",
       "4         insult                2011\n",
       "5  identity_hate                 357"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stats of X_test labels\n",
    "counts = []\n",
    "for i in labels:\n",
    "    counts.append((i, y_test[i].sum()))\n",
    "df_stats = pd.DataFrame(counts, columns=['Labels', 'number_of_comments'])\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text comments into vectors using bag of words or TF-IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that machine learning models doesn't accept input in the text format. So we need to convert the text data into Vector form, it is also called **Word Embeddings**. Word Embeddings can be broadly classified as:\n",
    "1. Frequency based - Most popular techniques are **Bag-of-Words**, **TF-IDF**\n",
    "2. Pridiction based - Most popular techniques are **Word2vec** and **Glove**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using **Bag-of-Words** and **TF-IDF**<br>\n",
    "<br>**Bag-of-Words(BOW)** - To get the embeddings from BOW we will firstly make a dictionary of words is from the test data along with the count of each word occurance in the data, then these words from the dictionary are sorted in descending order of their occurance,put these words into the columns and used as an independent features and here rows are the sentences or samples. These features will have values 0 or 1 based on if the word exists in the sentence.\n",
    "<br>\n",
    "**Disadvantage** of BOW - Word Embedding we get from BOW have either 0's and 1's as a values, no weights are given to the words according to their importance in the sentence. That means we can not get the sementics of the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF** - It stands for Term Frequency - Inverse Document Frequency\n",
    "<br> To get embedding with TF-IDF, we calculate Term frequency and Inverse Document Frequency seperate and then multiply them together to get TF-IDF.\n",
    "Formulas to calculate TF-IDF: <br>\n",
    "**TF** : $$\\frac{Number\\, of\\, repetition\\, of\\, word\\, in\\, a\\, sentence}{Number\\, of\\, words\\, in\\, a\\, sentence}$$ \n",
    "**IDF**:$$log\\Bigg[\\frac{Total\\, Number\\, of\\,sentences}{Number\\, of\\, sentences\\, containing\\, the \\, word}\\Bigg]$$ \n",
    "<br>\n",
    "**TF-IDF**: $$\\Bigg[\\frac{Number\\, of\\, repetition\\, of\\, word\\, in\\, a\\, sentence}{Number\\, of\\, words\\, in\\, a\\, sentence}\\Bigg]*log\\Bigg[\\frac{Total\\, Number\\, of\\,sentences}{Number\\, of\\, sentences\\, containing\\, the \\, word}\\Bigg]$$ \n",
    "<br>In **TF-IDF** also, we need dictionary of words with their count of occurance to do the calculation. **TF** assign more weightage to the word which repeat multiple times in the sentance where as **IDF** decreases the weightage to word as number of sentences containing the increases. Here, feature vectors not only contains 0's and 1' but does contain other other values depending on the word importance in the sentence. This is retaining the sementics of the sentence to some extent so it should perform better than BOW.\n",
    "<br>Here **TF-IDG** can have zero value for the word which existed in every sentence and give more weightage to less often occured words that means it could cause over-fitting problem but that is yet to discove. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xv_train, Xv_test = w_embeddings(X_train, X_test, \"tfidf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96     36078\n",
      "          1       0.94      0.16      0.28      3815\n",
      "\n",
      "avg / total       0.92      0.92      0.89     39893\n",
      "\n",
      "... Processing severe_toxic\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     39487\n",
      "          1       0.57      0.07      0.13       406\n",
      "\n",
      "avg / total       0.99      0.99      0.99     39893\n",
      "\n",
      "... Processing obscene\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     37750\n",
      "          1       0.96      0.26      0.41      2143\n",
      "\n",
      "avg / total       0.96      0.96      0.95     39893\n",
      "\n",
      "... Processing threat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     39788\n",
      "          1       0.00      0.00      0.00       105\n",
      "\n",
      "avg / total       0.99      1.00      1.00     39893\n",
      "\n",
      "... Processing insult\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     37882\n",
      "          1       0.80      0.18      0.29      2011\n",
      "\n",
      "avg / total       0.95      0.96      0.94     39893\n",
      "\n",
      "... Processing identity_hate\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     39536\n",
      "          1       0.00      0.00      0.00       357\n",
      "\n",
      "avg / total       0.98      0.99      0.99     39893\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Linear regression \n",
    "for label in labels:\n",
    "    print('... Processing {}'.format(label))\n",
    "    # train the model \n",
    "    logreg = OneVsRestClassifier(LogisticRegression(solver='sag'))\n",
    "    logreg.fit(Xv_train, y_train[label])\n",
    "    # compute the testing accuracy\n",
    "    prediction = logreg.predict(Xv_test)\n",
    "    print('Validation accuracy is \\n {}'.format(classification_report(y_test[label], prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> Checked the impact of use of Bag of words and TF-IDF on the accuracy of Linear regression. \n",
    "<br>Accuracy remained same - `Identity hate`, `threat`\n",
    "<br>Accuracy remained same - `Severe_toxic`\n",
    "<br>Accuracy improved little bit  with the use of TF-IDF but not very significant change for `Toxic`, `Obscene`, `insult`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Processing toxic\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      1.00      0.96     36078\n",
      "          1       0.99      0.21      0.35      3815\n",
      "\n",
      "avg / total       0.93      0.92      0.90     39893\n",
      "\n",
      "... Processing severe_toxic\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99     39487\n",
      "          1       0.00      0.00      0.00       406\n",
      "\n",
      "avg / total       0.98      0.99      0.98     39893\n",
      "\n",
      "... Processing obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     37750\n",
      "          1       0.97      0.33      0.49      2143\n",
      "\n",
      "avg / total       0.96      0.96      0.95     39893\n",
      "\n",
      "... Processing threat\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     39788\n",
      "          1       0.00      0.00      0.00       105\n",
      "\n",
      "avg / total       0.99      1.00      1.00     39893\n",
      "\n",
      "... Processing insult\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      1.00      0.98     37882\n",
      "          1       0.78      0.20      0.32      2011\n",
      "\n",
      "avg / total       0.95      0.96      0.94     39893\n",
      "\n",
      "... Processing identity_hate\n",
      "Validation accuracy is \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     39536\n",
      "          1       0.00      0.00      0.00       357\n",
      "\n",
      "avg / total       0.98      0.99      0.99     39893\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Naive bayes\n",
    "for label in labels:\n",
    "    print('... Processing {}'.format(label))\n",
    "    # train the model \n",
    "    nbayes = OneVsRestClassifier(naive_bayes.MultinomialNB())\n",
    "    nbayes.fit(Xv_train, y_train[label])\n",
    "    # compute the testing accuracy\n",
    "    prediction = nbayes.predict(Xv_test)\n",
    "    print('Validation accuracy is \\n {}'.format(classification_report(y_test[label], prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
