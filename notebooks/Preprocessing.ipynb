{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x11nOpWkhHyt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import nltk\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import re\r\n",
    "import string\r\n",
    "import operator\r\n",
    "from sklearn import feature_extraction,model_selection,naive_bayes,pipeline,manifold,preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8crPCeN5PchF",
    "outputId": "b0783981-2455-4a32-9278-afaf98b64bbc"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getrecursionlimit()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A6lJo6fMPev_"
   },
   "outputs": [],
   "source": [
    "# sys.setrecursionlimit(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vZnJ896ohoC-"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Input/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lgfCbNpXh7qj"
   },
   "outputs": [],
   "source": [
    "### data cleaner function\r\n",
    "def clean(input_str):\r\n",
    "    input_str = input_str.lower()\r\n",
    "    input_str = re.sub(r'\\d+', '', input_str)\r\n",
    "    input_str = re.sub(r\"n't\", \" not \", input_str)\r\n",
    "    input_str = re.sub(r\"can't\", \"cannot \", input_str)\r\n",
    "    input_str = re.sub(r\"what's\", \"what is \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'s\", \" \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'ve\", \" have \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'re\", \" are \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'d\", \" would \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'ll\", \" will \", input_str)\r\n",
    "    input_str = re.sub(r\"\\'scuse\", \" excuse \", input_str)\r\n",
    "    input_str = re.sub(r\"i'm\", \"i am\", input_str)\r\n",
    "    input_str = re.sub(r\" m \", \" am \", input_str)\r\n",
    "    input_str = re.sub('\\s+', ' ', input_str)\r\n",
    "    input_str = re.sub('\\W', ' ', input_str)\r\n",
    "    input_str = input_str.translate(str.maketrans('','', string.punctuation))\r\n",
    "    input_str = input_str.strip()\r\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BKDaQt-yiBo0"
   },
   "outputs": [],
   "source": [
    "### Frequently used words in the obscene comments\r\n",
    "def make_dict(d, stemm = False,lemm = True):\r\n",
    "    #all_word = []\r\n",
    "        ### Clean input data\r\n",
    "    processed_text = clean(d)\r\n",
    "        ### Tokenization\r\n",
    "    processed_text = word_tokenize(processed_text)\r\n",
    "     ### remove stop words\r\n",
    "    processed_text = [word for word in processed_text if word not in stopwords.words('english')]\r\n",
    "    #all_word.append(processed_text)\r\n",
    "\r\n",
    "    ### Stemming\r\n",
    "    if stemm == True:\r\n",
    "      ps = nltk.stem.porter.PorterStemmer()\r\n",
    "      processed_text = [ps.stem(word) for word in processed_text]\r\n",
    "\r\n",
    "    ### Lemmatization\r\n",
    "    if lemm == True:\r\n",
    "      lem = nltk.stem.wordnet.WordNetLemmatizer()\r\n",
    "      processed_text = [lem.lemmatize(word) for word in processed_text]\r\n",
    "\r\n",
    "    text = \" \".join(processed_text)\r\n",
    "    \r\n",
    "    return text\r\n",
    "\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zISYQ-DfiByc",
    "outputId": "8db0397b-e09e-4d84-efdf-a7ffe29a96e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey man realli tri edit war guy constantli remov relev inform talk edit instead talk page seem care format actual info'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = df.comment_text[2]\r\n",
    "\r\n",
    "make_dict(input_str, stemm= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append the clean comments in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tc9PsGd9iCGw"
   },
   "outputs": [],
   "source": [
    "# df['clean_comment'] = df['comment_text'].apply(lambda x:make_dict(x, stemm= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 924
    },
    "id": "UZNkUksQiCJJ",
    "outputId": "e8004035-4d87-4795-eaf9-97a46915a03a"
   },
   "outputs": [],
   "source": [
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TEaMUDIJPWip"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hax0tB4A8OmE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
