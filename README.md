
# Toxic Comment Classifier

It is been evident that human psychology is changing; our emotions are getting attached with the likes, comments and tags we receive on the social media. We receive both good and bad comment but the impact of toxic comments is affecting the engagement of the users in the meaningful conversations.
Moreover, users can see others using hateful words, slurs and ideas, and those things are becoming normal but the severity and viciousness of these comments has evolved into something much more sinister such as recent capitol riots at US parliament and attack on farmers by local goons in India.

## Inspiration
Sharp increase in hateful comments on the farmers who are protesting at the outskirts of India's capital Delhi since October 2020 against the 3 Farm Bills that has been passed by the government in September 2020. Due to rampant toxicity, people are avoiding having meaningful discussion on these bills. Therefore, having a solid toxicity flagging system in place will allow online forums and social media platforms to effectively facilitate conversations.

## Goal 
To build a Multi- label Classification model capable of detecting different level of toxicity like severe toxic, threats, obscenity, insults, and so on. 

## Dataset and Features
 <!-- Links -->
 [Dataset](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) used in this project is the Jigsaw/Conversation AI dataset provided for the Kaggle Toxic Comment Classification Challenge. This dataset contains 159571 Wikipedia comments which have been labeled by human raters for type of toxicity. This dataset can also be accessed from `data` folder, which contains the `train.csv` in `raw/` dir and processed data in `processed/` dir, [click here](https://drive.google.com/drive/folders/1gMJHNxCajYsRzMPjwUuPEM2S5tIp_b3r?usp=sharing) to download the `data ` folder and save it in the `Toxic Comment Classifier/` dir, this will let you run the code without having to worry about changing the path while reading the `.csv` data files.
<!-- UL -->
**train.csv** - this file contains comments with their binary labels which tells different type of toxicity. The types of toxicity are :
* `toxic`
* `severe_toxic`
* `obscene`
* `threat`
* `insult`
* `identity_hate`

* Random base line - discuss the class based accuracies
* Evaluation matrix

## Exploratory data analysis


## Data pre - processing


## ML models 

## Evaluation

